{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle as pk\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.backend import categorical_crossentropy, sparse_categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversational Model Metric\n",
    "def perplexity(y_true, y_pred):\n",
    "    return tf.pow(categorical_crossentropy(y_pred, y_true), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModelDirs(output_dir, model_dir, c=0):\n",
    "    '''Return model directories for outputing logs and checkpoints'''\n",
    "    final_dir = os.path.join(output_dir,model_dir+'_v{}'.format(c))\n",
    "    chpts_dir = os.path.join(final_dir,'chpts')\n",
    "    logs_dir = os.path.join(final_dir,'logs')  \n",
    "    \n",
    "    if model_dir+'_v{}'.format(c) in os.listdir(output_dir):\n",
    "        c += 1\n",
    "        final_dir, chpts_dir, logs_dir = createModelDirs(output_dir, model_dir, c)\n",
    "    else:\n",
    "        os.mkdir(final_dir)\n",
    "        os.mkdir(chpts_dir)\n",
    "        os.mkdir(logs_dir)\n",
    "    \n",
    "    return final_dir, chpts_dir, logs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'input/SBW-vectors-300-min5.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d89f8ac01eb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw2v_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'input/SBW-vectors-300-min5.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/TF_chatbot/chatbot_env/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1474\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1475\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/TF_chatbot/chatbot_env/lib/python3.6/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/TF_chatbot/chatbot_env/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mtransport_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_ext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_extension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransport_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mscrubbed_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/TF_chatbot/chatbot_env/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     )\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/TF_chatbot/chatbot_env/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input/SBW-vectors-300-min5.txt'"
     ]
    }
   ],
   "source": [
    "w2v_filename = 'input/SBW-vectors-300-min5.txt'\n",
    "w2v_model = KeyedVectors.load_word2vec_format(w2v_filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format(w2v_filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/data_cleaning_nlp/word_freqs_input.pk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ea36d990820e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_freqs_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/data_cleaning_nlp/word_freqs_input.pk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mword_freqs_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/data_cleaning_nlp/word_freqs_output.pk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/data_cleaning_nlp/input_data.pk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/data_cleaning_nlp/target_data.pk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/data_cleaning_nlp/word_freqs_input.pk'"
     ]
    }
   ],
   "source": [
    "# load variables\n",
    "word_freqs_inp = pk.load(open('output/data_cleaning_nlp/word_freqs_input.pk', 'rb'))\n",
    "word_freqs_out = pk.load(open('output/data_cleaning_nlp/word_freqs_output.pk', 'rb'))\n",
    "x = pk.load(open('output/data_cleaning_nlp/input_data.pk', 'rb'))\n",
    "y = pk.load(open('output/data_cleaning_nlp/target_data.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper-parameters\n",
    "# data features \n",
    "MAX_FEATURES_input = 1000\n",
    "input_len = 200\n",
    "MAX_FEATURES_output = 1000\n",
    "target_len = 200\n",
    "\n",
    "# training parameters\n",
    "num_epochs = 100\n",
    "batch_size = 32 # Buscar referencia Ian Goodfellow and Yan Le Cun\n",
    "\n",
    "# model estructure\n",
    "embed_size = 300\n",
    "hidden_size = 264\n",
    "n_encoder_layers = 3\n",
    "encoder_hidden_sizes = [256, 128, 64]\n",
    "n_decoder_layers = 3\n",
    "lstm_hidden_sizes = [256, 128, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output dir\n",
    "outDir = 'output/'\n",
    "actualDir = 'trained_model'\n",
    "\n",
    "print()\n",
    "if not(actualDir in os.listdir(outDir)):\n",
    "    os.mkdir(os.path.join(outDir, actualDir))\n",
    "    print('output dir created')\n",
    "else:\n",
    "    print('output dir already created')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories for outputs\n",
    "actual_outDir = os.path.join(outDir, actualDir)\n",
    "modelDir = 'model_epochs-{}_batch-{}_hidden-{}_embed-{}_PRUEBA'.format(num_epochs, batch_size, hidden_size, embed_size)\n",
    "finalDir, chptsDir, logsDir = createModelDirs(actual_outDir,modelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build vocabulary of unique words \n",
    "\n",
    "## Inputs\n",
    "vocab_size_input = min(MAX_FEATURES_input, len(word_freqs_inp)) + 4\n",
    "word2index_inp = {x[0]: i+4 for i, x in enumerate(word_freqs_inp.most_common(MAX_FEATURES_input))}\n",
    "word2index_inp[\"PAD\"] = 0\n",
    "word2index_inp[\"UNK\"] = 1\n",
    "word2index_inp[\"GO\"] = 2\n",
    "word2index_inp[\"EOS\"] = 3\n",
    "index2word_inp = {v:k for k, v in word2index_inp.items()}\n",
    "\n",
    "## Outputs\n",
    "vocab_size_output = min(MAX_FEATURES_output, len(word_freqs_out)) + 4\n",
    "word2index_out = {x[0]: i+4 for i, x in enumerate(word_freqs_out.most_common(MAX_FEATURES_output))}\n",
    "word2index_out[\"PAD\"] = 0\n",
    "word2index_out[\"UNK\"] = 1\n",
    "word2index_out[\"GO\"] = 2\n",
    "word2index_out[\"EOS\"] = 3\n",
    "index2word_out = {v:k for k, v in word2index_out.items()}\n",
    "\n",
    "# Save dictionaries in model directory\n",
    "pk.dump(word2index_inp, open(os.path.join(finalDir,'word2index_inp.pk'),'wb'))\n",
    "pk.dump(index2word_inp, open(os.path.join(finalDir,'index2word_inp.pk'),'wb'))\n",
    "pk.dump(word2index_out, open(os.path.join(finalDir,'word2index_out.pk'),'wb'))\n",
    "pk.dump(index2word_out, open(os.path.join(finalDir,'index2word_out.pk'),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros(shape=(300,len(index2word_inp)))\n",
    "\n",
    "for i, item in enumerate(index2word_inp.values()):\n",
    "    try: \n",
    "        vector = w2v_model.get_vector(item)\n",
    "    except:\n",
    "        vector = np.zeros(shape=300)\n",
    "    \n",
    "    embedding_matrix[i] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records by lenght \n",
    "x_new = []\n",
    "y_new = []\n",
    "\n",
    "for input_, target_ in zip(x,y):\n",
    "    if all([len(input_) <= input_len, len(input_) > 0, len(target_) <= target_len, len(target_) > 0]):\n",
    "        x_new.append(input_)\n",
    "        y_new.append(target_)\n",
    "        \n",
    "print('number of records after filtering by lenght:', len(x_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of conversations with the words replaced by their IDs\n",
    "X_input = np.empty((len(x_new),), dtype=list)\n",
    "y_input = np.empty((len(y_new),), dtype=list)\n",
    "y_target_ids = np.empty((len(y_new),), dtype=list)\n",
    "\n",
    "for i in range(len(x_new)):\n",
    "    seqs_x = []\n",
    "    seqs_y_input = []\n",
    "    seqs_y_target = []\n",
    "    \n",
    "    # Replace input sequences IDs\n",
    "    for word in x_new[i]:\n",
    "        if word in word2index_inp:\n",
    "            seqs_x.append(word2index_inp[word])\n",
    "        else:\n",
    "            seqs_x.append(word2index_inp[\"UNK\"]) # Replace words with low frequency with <UNK>\n",
    "               \n",
    "    # Target sequences IDs\n",
    "    seqs_y_input = [word2index_out[\"GO\"]] # Start of Sentence ID\n",
    "    for word in y_new[i]:\n",
    "        if word in word2index_out:\n",
    "            seqs_y_input.append(word2index_out[word])\n",
    "            seqs_y_target.append(word2index_out[word])\n",
    "        else:\n",
    "            # Replace words with low frequency with <UNK>\n",
    "            seqs_y_input.append(word2index_out[\"UNK\"])\n",
    "            seqs_y_target.append(word2index_out[\"UNK\"])\n",
    "    seqs_y_target.append(word2index_out[\"EOS\"]) # End of Sentece ID\n",
    "\n",
    "    X_input[i] = seqs_x\n",
    "    y_input[i] = seqs_y_input\n",
    "    y_target_ids[i] = seqs_y_target\n",
    "\n",
    "X_input = sequence.pad_sequences(X_input, input_len, padding='post')\n",
    "y_input = sequence.pad_sequences(y_input, target_len, padding='post')\n",
    "y_target_ids = sequence.pad_sequences(y_target_ids, target_len, padding='post')\n",
    "\n",
    "# Create one-hot target variable\n",
    "y_target = np.empty((len(y_target_ids), target_len, vocab_size_output))\n",
    "\n",
    "for i in range(len(y_target_ids)):\n",
    "    for j in range(target_len):\n",
    "        y_target[i, j, y_target_ids[i,j]] = 1\n",
    "        \n",
    "print(\"y_target size = %f gigabytes\" % ((y_target.size * y_target.itemsize)/1e9))\n",
    "\n",
    "# Save X and y input\n",
    "pk.dump(X_input, open(os.path.join(finalDir,'x_inp.pk'),'wb'))\n",
    "pk.dump(y_input, open(os.path.join(finalDir,'y_inp.pk'),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Tutorial seq2seq in keras\n",
    "#x = secuencia de indices (batch_size X seq_len)\n",
    "#y = secuencia de indices (batch_size X seq_len X embed_size)\n",
    "#target = secuencia de one-hot vectors para cada indice (batch_size X seq_len X vocab_size)\n",
    "\n",
    "## Alternativa para generar mas datos\n",
    "#x = secuencia de indices (batch_size X seq_len)\n",
    "#y = secuencia de indices hasta n (batch_size X seq_len X embed_size)\n",
    "#target = one-hot vector de la palabra n+1 (batch_size X vocab_size)\n",
    "\n",
    "def onehot(index):\n",
    "    vector = np.zeros((vocab_size_output))\n",
    "    vector[index] = 1\n",
    "    return vector\n",
    "\n",
    "window_size = 1\n",
    "\n",
    "Y = []\n",
    "X = []\n",
    "targets = []\n",
    "\n",
    "for ix, seq in tqdm(enumerate(y_input), total=y_input.shape[0]):\n",
    "    for i in range(len(seq)-1):\n",
    "        if seq[i] != 0:          \n",
    "            if i == 0:\n",
    "                X.append(X_input[ix])\n",
    "                y = seq[:i+window_size]\n",
    "                target = onehot(seq[i+window_size])\n",
    "                Y.append(y)\n",
    "                targets.append(target)\n",
    "            else:\n",
    "                if len(np.where(X_input[ix] == 0)[0]) > 1:\n",
    "                    X.append(np.insert(X_input[ix], np.where(X_input[ix] == 0)[0][0], seq[:i])[:200])\n",
    "                    y = seq[:i+window_size]\n",
    "                    target = onehot(seq[i+window_size])\n",
    "                    Y.append(y)\n",
    "                    targets.append(target)\n",
    "        else:\n",
    "            pass\n",
    "                   \n",
    "len(X), len(Y), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "Y = sequence.pad_sequences(Y, target_len, padding='post')\n",
    "targets = np.array(targets).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input.shape, y_input.shape, y_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape, targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recomendaciones para mejorar la calidad de los datos:\n",
    "\n",
    "1. Realizar lemmatizacion (spacy, clips-pattern.es)\n",
    "2. Usar diccionarios (lista de palabras)\n",
    "3. Usar word embeddings pre-entrenados y ajustarlos (vert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensorflow Keras Conversational Model \n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# Set up encoder, output lstm states\n",
    "encoder_embed_layer = Embedding(vocab_size_input, embed_size, mask_zero=True)\n",
    "encoder_embed = encoder_embed_layer(encoder_inputs)\n",
    "\n",
    "encoder_layers = [LSTM(encoder_hidden_sizes[i], return_sequences=True, go_backwards=True) for i in range(n_encoder_layers)]\n",
    "encoder_lstms_outputs = []\n",
    "for i in range(n_decoder_layers):\n",
    "    if i == 0:\n",
    "        encoder_lstms_outputs.append(encoder_layers[i](encoder_embed))\n",
    "    else:\n",
    "        encoder_lstms_outputs.append(encoder_layers[i](encoder_lstms_outputs[i-1]))\n",
    "\n",
    "encoder_lstm, state_h, state_c = LSTM(hidden_size, return_state=True,\n",
    "                                      go_backwards=True)(encoder_lstms_outputs[-1])\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "decoder_embed_layer = Embedding(vocab_size_output, embed_size, mask_zero=True)\n",
    "decoder_embed = decoder_embed_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embed, initial_state=encoder_states)\n",
    "\n",
    "decoder_layers = [LSTM(lstm_hidden_sizes[i], return_sequences=True) for i in range(n_decoder_layers)]\n",
    "decoder_lstms_outputs = []\n",
    "for i in range(n_decoder_layers):\n",
    "    if i == 0:\n",
    "        decoder_lstms_outputs.append(LSTM(lstm_hidden_sizes[i], return_sequences=True)(decoder_outputs))\n",
    "    elif i == n_decoder_layers-1:\n",
    "        decoder_lstms_outputs.append(LSTM(lstm_hidden_sizes[i], return_sequences=False)(decoder_lstms_outputs[i-1]))\n",
    "    else:\n",
    "        decoder_lstms_outputs.append(LSTM(lstm_hidden_sizes[i], return_sequences=True)(decoder_lstms_outputs[i-1]))\n",
    "\n",
    "# Create dense vector with next word probability \n",
    "decoder_dense = Dense(len(index2word_out), activation='relu')\n",
    "decoder_outputs = decoder_dense(decoder_lstms_outputs[-1])\n",
    "\n",
    "# Define the model that will turn 'X_input' and 'y_input' into 'y_target'\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# Compile model\n",
    "model.compile(optimizer=Nadam(), loss='sparse_categorical_crossentropy', metrics=[perplexity])\n",
    "# Model Estructure Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference Model\n",
    "# Encoder\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder inference inputs\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "decoder_states_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Decoder inference\n",
    "decoder_embed = decoder_embed_layer(decoder_inputs)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embed, initial_state=decoder_states_input)\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_layers = [LSTM(lstm_hidden_sizes[i], return_sequences=True) for i in range(n_decoder_layers)]\n",
    "decoder_lstms_outputs = []\n",
    "for i in range(n_decoder_layers):\n",
    "    if i == 0:\n",
    "        decoder_lstms_outputs.append(decoder_layers[i](decoder_outputs))\n",
    "    else:\n",
    "        decoder_lstms_outputs.append(decoder_layers[i](decoder_lstms_outputs[i-1]))\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_lstms_outputs[-1])\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_input,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Save models\n",
    "encoder_model.save(os.path.join(finalDir,'encoder_model_{}_{}_{}_{}.h5'.format(hidden_size,batch_size,num_epochs,embed_size)))\n",
    "decoder_model.save(os.path.join(finalDir,'decoder_model_{}_{}_{}_{}.h5'.format(hidden_size,batch_size,num_epochs,embed_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "model_checkpoint = ModelCheckpoint(os.path.join(chptsDir,'{epoch:02d}_{val_loss:.2f}.chpt'),\n",
    "                                   monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                                   save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=logsDir, histogram_freq=20, batch_size=32, write_graph=True,\n",
    "                          write_grads=True, embeddings_freq=0, embeddings_layer_names=None,\n",
    "                          embeddings_metadata=None, embeddings_data=None)\n",
    "\n",
    "# Fit model\n",
    "model_history = model.fit([X, Y], targets,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=num_epochs,\n",
    "                          validation_split=0.05,\n",
    "                          callbacks=[early_stopping, model_checkpoint, tensorboard])\n",
    "\n",
    "# Save model history\n",
    "with open(os.path.join(finalDir,'history_{}_{}_{}_{}.pk'.format(hidden_size,batch_size,num_epochs,embed_size)),'wb') as f:\n",
    "    pk.dump(model_history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
